{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "Before running this, make sure that all prerequisites are installed. This is done by running \n",
    "\n",
    "$ pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Bio as bio\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POSITIVE_TM_PATH = \"../data/training_data/positive_examples/tm\"\n",
    "POSITIVE_NON_TM_PATH = \"../data/training_data/positive_examples/non_tm\"\n",
    "NEGATIVE_TM_PATH = \"../data/training_data/negative_examples/tm\"\n",
    "NEGATIVE_NON_TM_PATH = \"../data/training_data/negative_examples/non_tm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_tm = [seq for path in os.listdir(POSITIVE_TM_PATH) \n",
    "                       for seq in SeqIO.parse(os.path.join(POSITIVE_TM_PATH, path), \"fasta\")]\n",
    "\n",
    "positive_non_tm = [seq for path in os.listdir(POSITIVE_NON_TM_PATH) \n",
    "                       for seq in SeqIO.parse(os.path.join(POSITIVE_NON_TM_PATH, path), \"fasta\")]\n",
    "\n",
    "negative_tm = [seq for path in os.listdir(NEGATIVE_TM_PATH) \n",
    "                       for seq in SeqIO.parse(os.path.join(NEGATIVE_TM_PATH, path), \"fasta\")]\n",
    "               \n",
    "negative_non_tm = [seq for path in os.listdir(NEGATIVE_NON_TM_PATH) \n",
    "                       for seq in SeqIO.parse(os.path.join(NEGATIVE_NON_TM_PATH, path), \"fasta\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives, tm: 45\n",
      "positives, non_tm: 1275\n",
      "negavtive, tm: 247\n",
      "negavtive, non_tm: 1087\n"
     ]
    }
   ],
   "source": [
    "print(\"positives, tm:\",  len(positive_tm))\n",
    "print(\"positives, non_tm:\",  len(positive_non_tm))\n",
    "print(\"negavtive, tm:\",  len(negative_tm))\n",
    "print(\"negavtive, non_tm:\", len( negative_non_tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "positive = positive_tm + positive_non_tm\n",
    "negative = negative_tm + negative_non_tm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives 1320\n",
      "negavtive 1334\n",
      "x shape (2654,)\n",
      "x shape (2654,)\n"
     ]
    }
   ],
   "source": [
    "print(\"positives\",  len(positive))\n",
    "print(\"negavtive\",  len(negative))\n",
    "x = np.array(positive + negative)\n",
    "y = np.array([1]*len(positive) + [0]*len(negative))\n",
    "\n",
    "assert x.shape == y.shape, \"There should be one lable for every datapoint\"\n",
    "print(\"x shape\",  x.shape)\n",
    "print(\"x shape\",  y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences to length 4563\n"
     ]
    }
   ],
   "source": [
    "X_train_seq, X_test_seq, Y_train, Y_test = cross_validation.train_test_split(x, y, random_state=7, train_size=0.9)\n",
    "\n",
    "def int_to_onehot(integer, length):\n",
    "    ret_v = q\n",
    "    \n",
    "    \n",
    "X_train = [[ord(c)-65 for c in str(seq.seq).split('#')[0].strip()] for seq in X_train_seq ]\n",
    "Z_train = [[ord(c)-65 for c in str(seq.seq).split('#')[0].strip()] for seq in X_train_seq ]\n",
    "X_test = [[ord(c)-65 for c in str(seq.seq).split('#')[0].strip()] for seq in X_test_seq ]\n",
    "Z_test = [[ord(c)-65 for c in str(seq.seq).split('#')[1].strip()] for seq in X_test_seq ]\n",
    "\n",
    "max_len = max([len(s) for s in X_train])\n",
    "top_ten = sorted([len(s) for s in X_train])[-100:]\n",
    "print(\"Padding sequences to length {}\".format(max_len))\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "Z_train_padded = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "Z_test_padded = sequence.pad_sequences(Z_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[820, 821, 822, 825, 830, 835, 844, 844, 844, 855, 857, 862, 872, 879, 880, 885, 886, 894, 894, 901, 907, 907, 908, 910, 917, 918, 919, 921, 929, 930, 947, 967, 969, 985, 988, 995, 999, 1001, 1010, 1015, 1015, 1019, 1023, 1023, 1028, 1034, 1039, 1040, 1045, 1049, 1053, 1065, 1070, 1071, 1072, 1077, 1086, 1098, 1101, 1116, 1119, 1164, 1167, 1210, 1231, 1245, 1247, 1257, 1315, 1367, 1401, 1426, 1448, 1473, 1474, 1476, 1524, 1531, 1562, 1581, 1608, 1663, 1669, 1670, 1782, 1807, 1820, 1826, 1853, 1886, 2148, 2179, 2201, 2303, 2332, 2386, 2750, 2813, 3084, 4563]\n"
     ]
    }
   ],
   "source": [
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 4563, 27)      729         embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (None, 50)            15600       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             51          lstm_8[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 16380\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "2388/2388 [==============================] - 1210s - loss: 0.6948 - acc: 0.5343   \n",
      "Epoch 2/5\n",
      "2388/2388 [==============================] - 1325s - loss: 0.6926 - acc: 0.5197   \n",
      "Epoch 3/5\n",
      "2388/2388 [==============================] - 1250s - loss: 0.6914 - acc: 0.5339   \n",
      "Epoch 4/5\n",
      "2388/2388 [==============================] - 1189s - loss: 0.6877 - acc: 0.5465  \n",
      "Epoch 5/5\n",
      "2388/2388 [==============================] - 1231s - loss: 0.6852 - acc: 0.5632   \n",
      "Accuracy: 58.27%\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "model = Sequential()\n",
    "model.add(Embedding(27, 27, input_length=max_len))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train_padded, Y_train, nb_epoch=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_padded, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "model_json = model.to_json()\n",
    "c_time = datetime.today().strftime(\"%d_%m_%Y_%I:%M%p\")\n",
    "with open(\"../results/{}.json\".format(c_time,  \"w+\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"../results/{}.h5\".format(c_time))\n",
    "print(\"Saved model to disk, timestamped with {}\".format(c_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7cbf95cce695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
